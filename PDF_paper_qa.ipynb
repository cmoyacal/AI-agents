{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Usepackages\n",
    "\n",
    "# standard\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "# langchain and rag\n",
    "from langchain import PromptTemplate\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain.llms import Ollama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Utils\n",
    "\n",
    "# class: supress stdout\n",
    "class SuppressStdout:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        self._original_stderr = sys.stderr\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "        sys.stderr = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "        sys.stderr = self._original_stderr\n",
    "\n",
    "# function: load pdf paper\n",
    "def load_pdf(file_path: str) -> Any:\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages = loader.load_and_split()\n",
    "    return pages\n",
    "\n",
    "# function: create vectorized database\n",
    "def create_vector_store(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    \n",
    "    embeddings = GPT4AllEmbeddings()\n",
    "    \n",
    "    with SuppressStdout():\n",
    "        vector_store = Chroma.from_documents(texts, embeddings)\n",
    "    return vector_store\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Config\n",
    "\n",
    "MODEL = \"mistral-nemo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /Users/cmoyacal/.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[44830]: Class GGMLMetalClass is implemented in both /Users/cmoyacal/miniforge3/envs/agents/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libreplit-mainline-metal.dylib (0x126e34208) and /Users/cmoyacal/miniforge3/envs/agents/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllamamodel-mainline-metal.dylib (0x127260208). One of the two will be used. Which one is undefined.\n",
      "/Users/cmoyacal/miniforge3/envs/agents/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, here are the key contributions of the paper:\n",
      "\n",
      "1. **LoRA-Ensemble**: The authors introduce LoRA-Ensemble, a method aimed at creating more efficient ensemble models for large language models (LLMs). This is done by freezing most of the LLM weights and only training a small number of low-rank matrices.\n",
      "2. **Efficient Epistemic Uncertainty Quantification**: The paper focuses on efficiently quantifying epistemic uncertainty in large machine learning models, which has been traditionally challenging due to intractable analytical computation.\n",
      "3. **Green AI Initiative**: The authors contribute to the concept of \"Green AI\" by aiming to reduce the computational resources and environmental impact associated with large LLMs. This is achieved through their efficient ensemble method that reduces training costs.\n",
      "4. **Comparison with Previous Methods**: The paper compares LoRA-Ensemble with previous methods, such as those proposed by Graves (2011), Blundell et al. (2015), Welling et al. (2011), and Lakshminarayanan et al. (2017). It suggests that probabilistic ensembles remain the best-performing approach for quantifying epistemic uncertainty, but LoRA-Ensemble offers a more efficient implementation.\n",
      "5. **Avoiding Correlation Between Ensemble Members**: The authors suggest using low-rank adaptations to achieve a low correlation between ensemble members, making their method distinct from naïve implementations of probabilistic ensembles.The authors estimate epistemic uncertainty using probabilistic ensembles. They train multiple independently trained models and interpret each model's predictions as Monte Carlo samples from the posterior weight space to obtain an unbiased estimator of the posterior distribution. The spread in predictions of these ensemble members serves as a measure of epistemic uncertainty. However, they acknowledge that for modern neural networks with billions of parameters, hardware restrictions render the naïve approach intractable due to increased computational cost and memory requirements.Based on the provided context, which appears to discuss a research paper about LoRA-Ensemble, a more efficient ensemble method for large-scale machine learning models, here are some potential limitations:\n",
      "\n",
      "1. **Limited Scope**: The paper focuses on fine-tuning Vision Transformers. While this is a significant contribution, the methods and findings might not directly apply to other types of models or architectures without further investigation.\n",
      "\n",
      "2. **Dependence on Fine-Tuning**: LoRA-Ensemble relies on fine-tuning pre-trained models. If the base model is poorly trained or unsuitable for the target task, performance may not improve significantly.\n",
      "\n",
      "3. **Resource Availability**: The paper aims to contribute to \"Green AI,\" but it still requires substantial computational resources for training and evaluation. Ensuring these resources are available and used responsibly could be a limitation in practice.\n",
      "\n",
      "4. **Limited Evaluation on Different Datasets or Tasks**: While the paper presents results on ImageNet, it might not have been evaluated on other datasets or tasks. This limits the generalizability of the findings to other scenarios.\n",
      "\n",
      "5. **Potential Overfitting**: The last layer initialization is varied in LoRA-Ensemble without adding noise. If not handled carefully, this could lead to overfitting and reduced generalization performance.\n",
      "\n",
      "6. **Lack of Comparison with State-of-the-Art Ensemble Methods**: Although the paper mentions comparison with other methods for Vision Transformers, it might not have been compared directly with state-of-the-art ensemble methods in general.The experiments described in this context involve the following key aspects:\n",
      "\n",
      "1. **Noise Injection**: The main experiment involves adding noise to the last layer initialization of neural networks trained on the CIFAR-100 dataset. This is done using various methods:\n",
      "   - Gaussian initialization with a mean of 0 and varying standard deviations.\n",
      "   - Xavier uniform initialization with different gain values.\n",
      "\n",
      "2. **Ensemble Models**: The experiments also compare results between single models and an ensemble of 16 members to see if there's any improvement in performance when combining multiple models.\n",
      "\n",
      "3. **Evaluation Metrics**: The primary metrics used for evaluation are accuracy and Expected Calibration Error (ECE).\n",
      "\n",
      "4. **Layer-wise Noise Addition**: A follow-up experiment is conducted where noise is added gradually, increasing the number of affected encoder layers starting from the last ones. Different noise scales (α ranging from 1 to 0.0001) are tested.\n",
      "\n",
      "5. **Default Initialization**: The last classification layer is initialized using PyTorch's default method for linear layers, as described by equations (6) and (7).Yes, noise is added during initialization in this context. The process involves adding noise to the last encoder layers of the model, with the goal of increasing diversity while maintaining pre-training. Different noise scales (α) were tried, ranging from 1 to 0.0001, and the experiment involved gradually increasing the number of affected encoder layers."
     ]
    }
   ],
   "source": [
    "#@title Main\n",
    "\n",
    "# you can use a local file path or a URL\n",
    "pdf_path = \"https://arxiv.org/pdf/2405.14438\"   \n",
    "\n",
    "# load the pdf\n",
    "documents = load_pdf(pdf_path)\n",
    "\n",
    "# create the vector store\n",
    "vectorstore = create_vector_store(documents)\n",
    "\n",
    "# qa while loop\n",
    "while True:\n",
    "    query = input(\"\\nQuery: \")\n",
    "    if query == \"exit\":\n",
    "        break\n",
    "    if query.strip() == \"\":\n",
    "        continue\n",
    "\n",
    "    # define the prompt\n",
    "    template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"\n",
    "\n",
    "    # define the qa chain prompt\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=template,\n",
    "    )\n",
    "\n",
    "    # define LLM\n",
    "    llm = Ollama(model=MODEL, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n",
    "\n",
    "    # define the qa chain\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "    )\n",
    "\n",
    "    result = qa_chain({\"query\": query})\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
